{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten, InputLayer, Conv1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent, CEMAgent, DDPGAgent, SARSAAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete\n",
    "\n",
    "from os.path import exists\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "from gym.utils.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TakeItEasyEnv(Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = Discrete(20, start=1)\n",
    "        self.observation_space = Box(low=-100, high=96100, shape=(1,1,20,3), dtype=np.int32)\n",
    "        self.state = np.zeros(shape=(20, 3), dtype=np.int32)\n",
    "        self.episode_length = 19\n",
    "        self.round_no = 0\n",
    "        self.pieces = np.array([\n",
    "            [2,1,3], [2,1,4], [2,1,8], [6,1,3], [6,1,4], [6,1,8], [7,1,3], [7,1,4], [7,1,8], \n",
    "            [2,5,3], [2,5,4], [2,5,8], [6,5,3], [6,5,4], [6,5,8], [7,5,3], [7,5,4], [7,5,8],\n",
    "            [2,9,3], [2,9,4], [2,9,8], [6,9,3], [6,9,4], [6,9,8], [7,9,3], [7,9,4], [7,9,8]])\n",
    "        self.cases = np.array([\n",
    "            [[1,4,8,0,0], [2,5,9,13,0], [3,6,10,14,17], [7,11,15,18,0], [12,16,19,0,0]],\n",
    "            [[1,2,3,0,0], [4,5,6,7,0], [8,9,10,11,12], [13,14,15,16,0], [17,18,19,0,0]], \n",
    "            [[3,7,12,0,0], [2,6,11,16,0], [1,5,10,15,19], [4,9,14,18,0], [8,13,17,0,0]]       \n",
    "            ], np.int32)\n",
    "        self.last_reward = 0\n",
    "        self.selected_pieces = rng.choice(self.pieces, size=self.episode_length, replace=False)\n",
    "\n",
    "\n",
    "        self.state[0] = self.selected_pieces[0]\n",
    "\n",
    "    def count_points(self, state):\n",
    "        points=0\n",
    "        for i in range(3):\n",
    "            for j in range(5):\n",
    "                first_value = state[self.cases[i][j][0],i]\n",
    "                for k in range(5):\n",
    "                    if(self.cases[i][j][k] == 0):\n",
    "                        points += first_value*k\n",
    "                        break\n",
    "                    if(self.state[self.cases[i][j][k], i] != first_value):\n",
    "                        break\n",
    "                    elif(k == 4):\n",
    "                        points += first_value*(k+1)\n",
    "                        break\n",
    "        return points\n",
    "\n",
    "    def reset(self):\n",
    "        self.round_no = 0\n",
    "        self.last_reward = 0\n",
    "        self.state=np.zeros(shape=(20, 3), dtype=np.int32)\n",
    "        self.selected_pieces = rng.choice(self.pieces, size=self.episode_length, replace=False)\n",
    "        self.state[0] = self.selected_pieces[0]\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.round_no += 1\n",
    "        reward = 0\n",
    "\n",
    "        if(np.all(self.state[action] != 0) and self.round_no <= self.episode_length):\n",
    "            reward = -100\n",
    "            done = True\n",
    "        else:\n",
    "            if(self.round_no == self.episode_length):\n",
    "                self.state[action] = self.state[0]\n",
    "                reward = 2*self.count_points(self.state) - self.last_reward\n",
    "                done = True\n",
    "            else:\n",
    "                self.state[action] = self.state[0]\n",
    "                self.state[0] = self.selected_pieces[self.round_no]\n",
    "                reward = self.count_points(self.state) - self.last_reward\n",
    "                self.last_reward = self.count_points(self.state)\n",
    "                done = False\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        return self.state, reward, done, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=TakeItEasyEnv()\n",
    "\n",
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(1,20,3)))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model\n",
    "\n",
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1, 20, 256)        1024      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1, 20, 512)        131584    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1, 20, 256)        131328    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 5120)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 20)                102420    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 366,356\n",
      "Trainable params: 366,356\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    memory = SequentialMemory(limit=200000000, window_length=1)\n",
    "    policy = BoltzmannQPolicy()\n",
    "    dqn = DQNAgent(model=model, nb_actions=actions, memory=memory, nb_steps_warmup=256, target_model_update=1e-2, batch_size = 128, policy=policy)\n",
    "    \n",
    "    return dqn\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "\n",
    "dqn.compile(Adam(learning_rate=0.5e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n",
      "Training for 10000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   83/10000 [..............................] - ETA: 18s - reward: 9.8795 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 526s 53ms/step - reward: 9.7450\n",
      "done, took 526.037 seconds\n",
      "Highest reward: 204.0\n",
      "Train reward: 185.2661596958175\n",
      "Model did not improve\n"
     ]
    }
   ],
   "source": [
    "def save_highscore(highscore):\n",
    "    with open('highscore.txt', 'w') as f:\n",
    "        f.write(str(highscore))\n",
    "\n",
    "def load_highscore():\n",
    "    try:\n",
    "        with open('highscore.txt', 'r') as f:\n",
    "            highscore = float(f.read())\n",
    "    except:\n",
    "        highscore = -100\n",
    "    return highscore\n",
    "\n",
    "steps = 5000\n",
    "highest_reward = load_highscore()\n",
    "for i in range(int(steps)//3000):\n",
    "    print('Step: {}'.format(i*3000))\n",
    "    try:\n",
    "        dqn.load_weights('dqn_take-it-easy_weights.hdf5')\n",
    "    except:\n",
    "        try:\n",
    "            dqn.load_weights('dqn_take-it-easy_weights-BACKUP.hdf5')\n",
    "        except:\n",
    "            print('Could not load weights')\n",
    "    \n",
    "    train_results = dqn.fit(env, nb_steps=10000, visualize=False, verbose=1)\n",
    "\n",
    "    results_reward = np.mean(train_results.history['episode_reward'])\n",
    "    print('Highest reward: {}'.format(highest_reward))\n",
    "    print('Train reward: {}'.format(results_reward))\n",
    "\n",
    "    if(results_reward > highest_reward):\n",
    "        print('Model improved')\n",
    "        dqn.save_weights('dqn_take-it-easy_weights.hdf5', overwrite=True)\n",
    "        highest_reward = results_reward\n",
    "        save_highscore(highest_reward)\n",
    "    else:\n",
    "        print('Model did not improve')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
